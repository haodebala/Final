{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "import dateutil.relativedelta\n",
    "import time\n",
    "import csv\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_api import articleAPI\n",
    "api = articleAPI(\"S49SxPkHv9vcW0RyJqVyM7fCASbkThWL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20001126'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_calculation():\n",
    "    timeNow = datetime.now()\n",
    "    timeNow2=timeNow.strftime(\"%Y-%m-%d\")\n",
    "    d = datetime.strptime(timeNow2, \"%Y-%m-%d\")\n",
    "    d2 = d - dateutil.relativedelta.relativedelta(years=20)\n",
    "    d3 = d2.strftime(\"%Y%m%d\")\n",
    "    return(d3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_articles(a):\n",
    "    allArticles = []\n",
    "    allSubjects = {}\n",
    "    for i in range(0,100):                  \n",
    "        articles = api.search(q=a,\\\n",
    "            fq={\"source\":[\"The New York Times\",\"Reuters\",\"AP\"]},\\\n",
    "            page=i,\\\n",
    "            begin_date=time_calculation(), \n",
    "            sort = \"newest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_articles(articles):\n",
    "    compiled= []\n",
    "    for i in articles[\"response\"][\"docs\"]:\n",
    "        time.sleep(0.5)  \n",
    "        dictA = {}\n",
    "        dictA[\"headline\"]=i[\"headline\"][\"main\"]\n",
    "        if i[\"snippet\"] is not None:\n",
    "            dictA[\"snippet\"] = i[\"snippet\"]\n",
    "        dictA[\"date\"]=i[\"pub_date\"][0:10]\n",
    "        dictA[\"link\"] = i[\"web_url\"]\n",
    "        compiled.append(dictA)\n",
    "    return(compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_articles(a):\n",
    "    allArticles = []\n",
    "    allSubjects = {}\n",
    "    for i in range(0,100):\n",
    "        articles = api.search(q=a,\\\n",
    "            page=i,\\\n",
    "            begin_date=time_calculation(),\\\n",
    "            sort = \"newest\")\n",
    "        if articles[\"response\"][\"docs\"] != []: \n",
    "            for j in articles[\"response\"][\"docs\"]:\n",
    "                time.sleep(0.5)\n",
    "                for x in range(0,len(j['keywords'])):\n",
    "                    if 'subject' in j['keywords'][x]['name']:\n",
    "                        if j['keywords'][x]['value'] not in allSubjects.keys():\n",
    "                            allSubjects[j['keywords'][x]['value']] = 1\n",
    "                        else:\n",
    "                            allSubjects[j['keywords'][x]['value']] = \\\n",
    "                            allSubjects[j['keywords'][x]['value']] + 1\n",
    "            articles = compile_articles(articles)                                                               \n",
    "            allArticles = allArticles + articles\n",
    "        else:\n",
    "            break\n",
    "    row = []\n",
    "    allSubjects = OrderedDict(sorted(allSubjects.items(), key=lambda d: d[1],reverse = True))     \n",
    "    for i in range(len(allSubjects)):\n",
    "        B= {}\n",
    "        B[\"Subjects\"] = list(allSubjects.keys())[i]\n",
    "        B[\"times\"] = allSubjects[list(allSubjects.keys())[i]]\n",
    "        row.append(B)\n",
    "    intoCsv(allArticles,row)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intoCsv(a,b):\n",
    "    with open(\"theDoc1.csv\",\"w\",encoding='utf-8') as fOut:\n",
    "        csvOut = csv.DictWriter(fOut, \n",
    "        [\"date\",\"headline\",\"snippet\",\"link\"])\n",
    "        csvOut.writeheader()\n",
    "        csvOut.writerows(a)\n",
    "    \n",
    "    with open(\"theStatistics1.csv\",\"w\",encoding='utf-8') as fOut:\n",
    "        csvOut = csv.DictWriter(fOut,[\"Subjects\",\"times\"])\n",
    "        csvOut.writeheader()\n",
    "        csvOut.writerows(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "getting_articles('Tropical Cyclones And Global Warming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  time\n",
      "0   2020    11\n",
      "1   2019    15\n",
      "2   2018    10\n",
      "3   2017     8\n",
      "4   2016     7\n",
      "5   2015     3\n",
      "6   2014     5\n",
      "7   2013     6\n",
      "8   2012     8\n",
      "9   2011     7\n",
      "10  2010     3\n",
      "11  2009     2\n",
      "12  2008     9\n",
      "13  2007     5\n",
      "14  2006     9\n",
      "15  2005     4\n",
      "16  2004     1\n",
      "17  2003     1\n",
      "18  2002     1\n",
      "19  2001     3\n"
     ]
    }
   ],
   "source": [
    "import  pandas as pd\n",
    "f=pd.read_csv('C:\\\\Users\\\\User\\\\Downloads\\\\semA2020-2021\\\\COM3203\\\\theDoc1.csv')\n",
    "date=list(f['date'].values)\n",
    "temp=[]\n",
    "pinci={}\n",
    "for i in date:\n",
    "    year=i[0:4]\n",
    "    if year not in temp:\n",
    "        temp.append(year)\n",
    "        pinci.update({year:1})\n",
    "    else:\n",
    "        pinci[year]+=1\n",
    "pincidata=pd.DataFrame({'year':list(pinci.keys()),'time':list(pinci.values())})\n",
    "pincidata.to_csv(\"result.csv\")\n",
    "print(pincidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "f2=pd.read_csv('C:\\\\Users\\\\User\\\\Downloads\\\\semA2020-2021\\\\COM3203\\\\theStatistics1.csv')\n",
    "sub=list(f2['Subjects'].values)\n",
    "times=list(f2['times'].values)\n",
    "content=''\n",
    "for i in range(0,len(sub)):\n",
    "    content+=(sub[i]+'\\\\')*times[i]\n",
    "wordcloud = WordCloud(background_color='#1F2630',\n",
    "                      width=1000,\n",
    "                      height=860,\n",
    "                      max_words=200,\n",
    "                      collocations=False).generate(content)\n",
    "\n",
    "image_produce = wordcloud.to_image()\n",
    "image_produce.show()\n",
    "image_produce.save(\"ciyun.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
