{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CRAWLING\n",
    "\n",
    "When we researched what factors influence tropical cyclone, it was found that there is some thesis stating that tropical cyclone is related to global warming. Therefore we try to scrape news articles from The New York Times to find if there is any connection between them. So The data for scraping comes from THE NEW YORK TIMES by using the search_API, with the help of dateutil which is our major package.\n",
    "\n",
    "The articles about the tropical cyclone and global warming from 2000 to 2020 are to be scrapped. And the variables are the time which is 20 years and the keyword which is tropical cyclone and global warming. The results of the scrapc can stimulate our further research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the headline, snippet, date, and link of every article from 2000 to 2020:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "import dateutil.relativedelta\n",
    "import time\n",
    "import csv\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_api import articleAPI\n",
    "api = articleAPI(\"S49SxPkHv9vcW0RyJqVyM7fCASbkThWL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_calculation():\n",
    "    timeNow = datetime.now()\n",
    "    timeNow2=timeNow.strftime(\"%Y-%m-%d\")\n",
    "    d = datetime.strptime(timeNow2, \"%Y-%m-%d\")\n",
    "    d2 = d - dateutil.relativedelta.relativedelta(years=20)\n",
    "    d3 = d2.strftime(\"%Y%m%d\")\n",
    "    return(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_articles(a):\n",
    "    allArticles = []\n",
    "    allSubjects = {}\n",
    "    for i in range(0,100):                   \n",
    "        articles = api.search(q=a,\\\n",
    "            fq={\"source\":[\"The New York Times\",\"Reuters\",\"AP\"]},\\\n",
    "            page=i,\\\n",
    "            begin_date=time_calculation(),  \n",
    "            sort = \"newest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_articles(articles):\n",
    "    compiled= []\n",
    "    for i in articles[\"response\"][\"docs\"]:\n",
    "        time.sleep(0.5)  \n",
    "        dictA = {}\n",
    "        dictA[\"headline\"]=i[\"headline\"][\"main\"]\n",
    "        if i[\"snippet\"] is not None:\n",
    "            dictA[\"snippet\"] = i[\"snippet\"]\n",
    "        dictA[\"date\"]=i[\"pub_date\"][0:10]\n",
    "        dictA[\"link\"] = i[\"web_url\"]\n",
    "        compiled.append(dictA)\n",
    "    return(compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_articles(a):\n",
    "    allArticles = []\n",
    "    allSubjects = {}\n",
    "    for i in range(0,100):\n",
    "        articles = api.search(q=a,\\\n",
    "            page=i,\\\n",
    "            begin_date=time_calculation(),\\\n",
    "            sort = \"newest\")\n",
    "        if articles[\"response\"][\"docs\"] != []: \n",
    "            for j in articles[\"response\"][\"docs\"]:\n",
    "                time.sleep(0.5)\n",
    "                for x in range(0,len(j['keywords'])):\n",
    "                    if 'subject' in j['keywords'][x]['name']:\n",
    "                        if j['keywords'][x]['value'] not in allSubjects.keys():\n",
    "                            allSubjects[j['keywords'][x]['value']] = 1\n",
    "                        else:\n",
    "                            allSubjects[j['keywords'][x]['value']] = \\\n",
    "                            allSubjects[j['keywords'][x]['value']] + 1\n",
    "            articles = compile_articles(articles)                                                               \n",
    "            allArticles = allArticles + articles\n",
    "        else:\n",
    "            break\n",
    "    row = []\n",
    "    allSubjects = OrderedDict(sorted(allSubjects.items(), key=lambda d: d[1],reverse = True))     \n",
    "    for i in range(len(allSubjects)):\n",
    "        B= {}\n",
    "        B[\"Subjects\"] = list(allSubjects.keys())[i]\n",
    "        B[\"times\"] = allSubjects[list(allSubjects.keys())[i]]\n",
    "        row.append(B)\n",
    "    intoCsv(allArticles,row)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data set is stored in CSV format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intoCsv(a,b):\n",
    "    with open(\"theDoc1.csv\",\"w\",encoding='utf-8') as fOut:\n",
    "        csvOut = csv.DictWriter(fOut, \n",
    "        [\"date\",\"headline\",\"snippet\",\"link\"])\n",
    "        csvOut.writeheader()\n",
    "        csvOut.writerows(a)\n",
    "    \n",
    "    with open(\"theStatistics1.csv\",\"w\",encoding='utf-8') as fOut:\n",
    "        csvOut = csv.DictWriter(fOut,[\"Subjects\",\"times\"])\n",
    "        csvOut.writeheader()\n",
    "        csvOut.writerows(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "getting_articles('Tropical Cyclones And Global Warming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the difficulties, at first, we tried to scrape from the NEW YORK TIMES, but we can not get the document. And we changed to scrape data from google news, but it has an anti-crawler mechanism. So we try to use new york times API again, with the help of Chinese Software Developer Network (CSDN), we finally knew that there were some problems when I installed new york times article API. After changing it to install search API, it works. Therefore we can get the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING\n",
    "\n",
    "When the data are available, it is clear that articles about \"tropical cyclone and global warming\" has increased in recent years.To make the data more obvious, the data is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  time\n",
      "0   2020    11\n",
      "1   2019    15\n",
      "2   2018    10\n",
      "3   2017     8\n",
      "4   2016     7\n",
      "5   2015     3\n",
      "6   2014     5\n",
      "7   2013     6\n",
      "8   2012     8\n",
      "9   2011     7\n",
      "10  2010     3\n",
      "11  2009     2\n",
      "12  2008     9\n",
      "13  2007     5\n",
      "14  2006     9\n",
      "15  2005     4\n",
      "16  2004     1\n",
      "17  2003     1\n",
      "18  2002     1\n",
      "19  2001     3\n"
     ]
    }
   ],
   "source": [
    "import  pandas as pd\n",
    "f=pd.read_csv('C:\\\\Users\\\\User\\\\Downloads\\\\semA2020-2021\\\\COM3203\\\\theDoc1.csv')\n",
    "date=list(f['date'].values)\n",
    "temp=[]\n",
    "pinci={}\n",
    "for i in date:\n",
    "    year=i[0:4]\n",
    "    if year not in temp:\n",
    "        temp.append(year)\n",
    "        pinci.update({year:1})\n",
    "    else:\n",
    "        pinci[year]+=1\n",
    "pincidata=pd.DataFrame({'year':list(pinci.keys()),'time':list(pinci.values())})\n",
    "pincidata.to_csv(\"result.csv\")\n",
    "print(pincidata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the data set \"theStatistics1\", there are many words related to \"tropical cyclone and global warming\" in these articles. In addition to \"global warming\", \"storms\", there are many words that appear multiple times, such as \"weather\", \"flood\" and so on. This data set will be transformed into a wordcloud for our further research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the wordcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "f2=pd.read_csv('C:\\\\Users\\\\User\\\\Downloads\\\\semA2020-2021\\\\COM3203\\\\theStatistics1.csv')\n",
    "sub=list(f2['Subjects'].values)\n",
    "times=list(f2['times'].values)\n",
    "content=''\n",
    "for i in range(0,len(sub)):\n",
    "    content+=(sub[i]+'\\\\')*times[i]\n",
    "wordcloud = WordCloud(background_color='#1F2630',\n",
    "                      width=1000,\n",
    "                      height=860,\n",
    "                      max_words=200,\n",
    "                      collocations=False).generate(content)\n",
    "\n",
    "image_produce = wordcloud.to_image()\n",
    "image_produce.show()\n",
    "image_produce.save(\"ciyun.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this wordcloud, it is obvious that tropical cyclone will appear with floods, greenhouse gas emmisions, environment and so on, which means that in addition to being linked to global warming, tropical cyclone may also be linked to floods, greenhouse gas, environment. It gives us an inspiration to do further research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
